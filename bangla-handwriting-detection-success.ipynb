{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10466688,"sourceType":"datasetVersion","datasetId":6410884},{"sourceId":10474973,"sourceType":"datasetVersion","datasetId":6486000}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y dask\n!pip install dask==2024.8.0\n!pip install -q datasets==2.21.0 jiwer evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"veBpuuVCdPDY","outputId":"955669fe-d0b3-4128-b0f0-5e1d7d0ba645","execution":{"iopub.status.busy":"2025-03-13T08:27:18.234807Z","iopub.execute_input":"2025-03-13T08:27:18.235070Z","iopub.status.idle":"2025-03-13T08:27:31.395376Z","shell.execute_reply.started":"2025-03-13T08:27:18.235048Z","shell.execute_reply":"2025-03-13T08:27:31.394476Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: dask 2024.12.1\nUninstalling dask-2024.12.1:\n  Successfully uninstalled dask-2024.12.1\nCollecting dask==2024.8.0\n  Downloading dask-2024.8.0-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask==2024.8.0) (8.1.7)\nRequirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.8.0) (3.1.0)\nRequirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.8.0) (2024.6.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.8.0) (24.1)\nRequirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.8.0) (1.4.2)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask==2024.8.0) (6.0.2)\nRequirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.8.0) (0.12.1)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.8.0) (8.5.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask==2024.8.0) (3.20.2)\nRequirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask==2024.8.0) (1.0.0)\nDownloading dask-2024.8.0-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: dask\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-expr 1.1.21 requires dask==2024.12.1, but you have dask 2024.8.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dask-2024.8.0\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-expr 1.1.21 requires dask==2024.12.1, but you have dask 2024.8.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers==4.45.2 sentence-transformers==3.1.1","metadata":{"id":"r3gfoTpOdumJ","outputId":"599ba4a1-42bc-44e1-b776-2f24a8ace48e","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:27:31.396248Z","iopub.execute_input":"2025-03-13T08:27:31.396485Z","iopub.status.idle":"2025-03-13T08:27:45.118229Z","shell.execute_reply.started":"2025-03-13T08:27:31.396466Z","shell.execute_reply":"2025-03-13T08:27:45.117139Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.45.2\n  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sentence-transformers==3.1.1\n  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.4.5)\nCollecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (4.66.5)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.1.1) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.1.1) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.1.1) (1.13.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.1.1) (10.4.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.1.1) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.1.1) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.1.1) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2024.8.30)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==3.1.1) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==3.1.1) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.1.1) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers==3.1.1) (1.3.0)\nDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers, sentence-transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.2\n    Uninstalling transformers-4.44.2:\n      Successfully uninstalled transformers-4.44.2\nSuccessfully installed sentence-transformers-3.1.1 tokenizers-0.20.3 transformers-4.45.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import default_data_collator\nfrom transformers import ViTFeatureExtractor, RobertaTokenizer, XLMRobertaTokenizer, TrOCRProcessor, BertTokenizer\nfrom transformers import VisionEncoderDecoderModel\nfrom transformers import TrOCRProcessor\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments","metadata":{"trusted":true,"id":"0UnMAKsZdPDb","execution":{"iopub.status.busy":"2025-03-13T08:27:45.119208Z","iopub.execute_input":"2025-03-13T08:27:45.119434Z","iopub.status.idle":"2025-03-13T08:27:59.287973Z","shell.execute_reply.started":"2025-03-13T08:27:45.119417Z","shell.execute_reply":"2025-03-13T08:27:59.287285Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"os.environ[\"WANDB_PROJECT\"] = \"trocr-handwriting\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:27:59.289602Z","iopub.execute_input":"2025-03-13T08:27:59.290061Z","iopub.status.idle":"2025-03-13T08:27:59.293716Z","shell.execute_reply.started":"2025-03-13T08:27:59.290039Z","shell.execute_reply":"2025-03-13T08:27:59.292788Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_dataset(json_path):\n    # Load the dataset\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    \n    # More thorough filtering\n    temp = []\n    skipped_count = 0\n    for d in data:\n        # Clean the text thoroughly\n        text = d['text'].strip()\n        \n        # Skip invalid samples\n        if not text or text.isspace() or len(text) == 0:\n            skipped_count += 1\n            print(f\"Skipping sample {d['file']} due to invalid text: '{text}'\")\n            continue\n            \n        # Skip specific problematic files if needed\n        if d['file'] == '100_8_15_8':\n            skipped_count += 1\n            print(f\"Skipping sample {d['file']} as it is explicitly excluded\")\n            continue\n            \n        # Add valid samples to the list\n        temp.append({\n            'file': d['file'],\n            'text': text  # Store the cleaned text\n        })\n    \n    # Create and validate DataFrame\n    df = pd.DataFrame(temp)\n    \n    print(f\"Original dataset size: {len(data)}\")\n    print(f\"Filtered dataset size: {len(df)}\")\n    print(f\"Skipped {skipped_count} samples\")\n    \n    # Final validation\n    assert not df['text'].isna().any(), \"Found NaN values in text column\"\n    assert not (df['text'].str.len() == 0).any(), \"Found empty strings after filtering\"\n    \n    return df\n\ndf = load_dataset('/kaggle/input/handwriting-dataset-v3/annotation.json')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:27:59.297054Z","iopub.execute_input":"2025-03-13T08:27:59.297357Z","iopub.status.idle":"2025-03-13T08:27:59.415659Z","shell.execute_reply.started":"2025-03-13T08:27:59.297328Z","shell.execute_reply":"2025-03-13T08:27:59.414786Z"}},"outputs":[{"name":"stdout","text":"Original dataset size: 20579\nFiltered dataset size: 20579\nSkipped 0 samples\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:27:59.416592Z","iopub.execute_input":"2025-03-13T08:27:59.416863Z","iopub.status.idle":"2025-03-13T08:27:59.422410Z","shell.execute_reply.started":"2025-03-13T08:27:59.416841Z","shell.execute_reply":"2025-03-13T08:27:59.421273Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"20579"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:27:59.423351Z","iopub.execute_input":"2025-03-13T08:27:59.423603Z","iopub.status.idle":"2025-03-13T08:27:59.443792Z","shell.execute_reply.started":"2025-03-13T08:27:59.423563Z","shell.execute_reply":"2025-03-13T08:27:59.443013Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForMaskedLM, BertTokenizer, BertModel\n\nmodel_id = \"microsoft/trocr-small-handwritten\"\nencode = 'google/vit-base-patch16-224'\ndecode = 'google-bert/bert-base-multilingual-cased' # Bert er custom fine tune model\n\nprocessor = TrOCRProcessor.from_pretrained(model_id)\nmodel = VisionEncoderDecoderModel.from_pretrained(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:27:59.444727Z","iopub.execute_input":"2025-03-13T08:27:59.444993Z","iopub.status.idle":"2025-03-13T08:28:04.808715Z","shell.execute_reply.started":"2025-03-13T08:27:59.444967Z","shell.execute_reply":"2025-03-13T08:28:04.808106Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe120494cd14e51a8a66c27b07589dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/327 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e551ac9610e422185928f081d93024a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e51802502cb473187dadc2e4b8c3521"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a74b00899de4423ab89993b9ef8c62d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b7ca00894614f55ac9837bf20c4dae6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2bf11b8420b4abd92bbbdf30cf6f5c4"}},"metadata":{}},{"name":"stderr","text":"VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2c864297a5f4b019eeae03bf4b210be"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel.to(device)\nprint(model)\n\n# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\n\ntotal_trainable_params = sum(\n   p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"{total_trainable_params:,} training parameters.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:28:04.809765Z","iopub.execute_input":"2025-03-13T08:28:04.810014Z","iopub.status.idle":"2025-03-13T08:28:05.171437Z","shell.execute_reply.started":"2025-03-13T08:28:04.809994Z","shell.execute_reply":"2025-03-13T08:28:05.170698Z"}},"outputs":[{"name":"stdout","text":"VisionEncoderDecoderModel(\n  (encoder): DeiTModel(\n    (embeddings): DeiTEmbeddings(\n      (patch_embeddings): DeiTPatchEmbeddings(\n        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): DeiTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x DeiTLayer(\n          (attention): DeiTAttention(\n            (attention): DeiTSelfAttention(\n              (query): Linear(in_features=384, out_features=384, bias=True)\n              (key): Linear(in_features=384, out_features=384, bias=True)\n              (value): Linear(in_features=384, out_features=384, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): DeiTSelfOutput(\n              (dense): Linear(in_features=384, out_features=384, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): DeiTIntermediate(\n            (dense): Linear(in_features=384, out_features=1536, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DeiTOutput(\n            (dense): Linear(in_features=1536, out_features=384, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n    (pooler): DeiTPooler(\n      (dense): Linear(in_features=384, out_features=384, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): TrOCRScaledWordEmbedding(64044, 256, padding_idx=1)\n        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0-5): 6 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n  )\n)\n61,596,672 total parameters.\n61,596,672 training parameters.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\nmodel.config.vocab_size = processor.tokenizer.vocab_size\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\n\nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:28:05.172183Z","iopub.execute_input":"2025-03-13T08:28:05.172396Z","iopub.status.idle":"2025-03-13T08:28:05.176661Z","shell.execute_reply.started":"2025-03-13T08:28:05.172379Z","shell.execute_reply":"2025-03-13T08:28:05.175995Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import evaluate\n\ncer_metric = evaluate.load('cer')\n\ndef compute_cer(pred):\n    # Extract label and prediction IDs\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n    \n    # Replace -100 with pad token ID in labels\n    labels_ids = [\n        [token if token != -100 else processor.tokenizer.pad_token_id for token in label]\n        for label in labels_ids\n    ]\n    \n    # Decode predictions and labels\n    pred_words = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    label_words = processor.batch_decode(labels_ids, skip_special_tokens=True)\n    \n    # Clean and validate the strings\n    cleaned_pred_words = []\n    cleaned_label_words = []\n    \n    for pred_word, label_word in zip(pred_words, label_words):\n        # Strip whitespace\n        pred_word = pred_word.strip()\n        label_word = label_word.strip()\n        \n        # Use a single space for empty strings\n        if not pred_word:\n            pred_word = \" \"\n        if not label_word:\n            label_word = \" \"\n        \n        cleaned_pred_words.append(pred_word)\n        cleaned_label_words.append(label_word)\n    \n    # Compute CER\n    try:\n        cer = cer_metric.compute(predictions=cleaned_pred_words, references=cleaned_label_words)\n        return {\"cer\": cer}\n    except Exception as e:\n        print(f\"Error computing CER: {str(e)}\")\n        return {\"cer\": 1.0}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:28:05.177526Z","iopub.execute_input":"2025-03-13T08:28:05.177794Z","iopub.status.idle":"2025-03-13T08:28:05.826703Z","shell.execute_reply.started":"2025-03-13T08:28:05.177761Z","shell.execute_reply":"2025-03-13T08:28:05.825839Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2075be2d178b418c8711a1a49f8a15c8"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"class IAMDataset(Dataset):\n    def __init__(self, root_dir, df, processor, max_target_length=128):\n        self.root_dir = root_dir\n        # Filter out any rows with empty text\n        self.df = df[df['text'].str.strip().str.len() > 0].reset_index(drop=True)\n        self.processor = processor\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.df['file'].iloc[idx]\n        text = self.df['text'].iloc[idx].strip()\n        \n        if not text:\n            text = \" \"\n            \n        image = Image.open(f'{self.root_dir}/{file_name}').convert(\"RGB\")\n\n        # Process image\n        pixel_values = self.processor(\n            image,\n            return_tensors=\"pt\"\n        ).pixel_values.squeeze(0)\n\n        # print(f\"pixel_values shape: {pixel_values.shape}\")\n\n        # Process text with matching sequence length\n        encoding = self.processor.tokenizer(\n            text,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n            max_length=self.max_target_length\n        )\n\n        labels = encoding.input_ids.squeeze()\n        \n        # Replace padding tokens with -100 for loss calculation\n        # labels[labels == self.processor.tokenizer.pad_token_id] = -100\n        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n\n        return {\n            \"pixel_values\": pixel_values,\n            \"labels\": labels\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:28:05.829211Z","iopub.execute_input":"2025-03-13T08:28:05.829429Z","iopub.status.idle":"2025-03-13T08:28:05.835714Z","shell.execute_reply.started":"2025-03-13T08:28:05.829410Z","shell.execute_reply":"2025-03-13T08:28:05.834894Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_data = IAMDataset(\n    root_dir='/kaggle/input/handwriting-dataset-v3/content/output',\n    df=train_df,\n    processor=processor\n)\nvalid_data = IAMDataset(\n    root_dir='/kaggle/input/handwriting-dataset-v3/content/output',\n    df=valid_df,\n    processor=processor\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:28:05.836876Z","iopub.execute_input":"2025-03-13T08:28:05.837143Z","iopub.status.idle":"2025-03-13T08:28:05.874125Z","shell.execute_reply.started":"2025-03-13T08:28:05.837124Z","shell.execute_reply":"2025-03-13T08:28:05.873100Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:28:05.875085Z","iopub.execute_input":"2025-03-13T08:28:05.875359Z","iopub.status.idle":"2025-03-13T08:28:05.963670Z","shell.execute_reply.started":"2025-03-13T08:28:05.875330Z","shell.execute_reply":"2025-03-13T08:28:05.962775Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n \n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n \n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n 'labels': [tensor(0),\n  tensor(190),\n  tensor(63014),\n  tensor(62952),\n  tensor(63408),\n  tensor(62866),\n  tensor(3),\n  tensor(57860),\n  tensor(62897),\n  tensor(62856),\n  tensor(62830),\n  tensor(2),\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100]}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    num_train_epochs=10,\n    predict_with_generate=True,\n    eval_strategy=\"epoch\",\n    save_strategy='epoch',\n    save_total_limit=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    learning_rate=5e-5,\n    fp16=True,\n    output_dir=\"/kaggle/working/\",\n    logging_steps=10,\n    eval_steps=100,\n    run_name='trocr-small-handwritten-v1'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:28:05.964581Z","iopub.execute_input":"2025-03-13T08:28:05.964952Z","iopub.status.idle":"2025-03-13T08:28:05.995376Z","shell.execute_reply.started":"2025-03-13T08:28:05.964902Z","shell.execute_reply":"2025-03-13T08:28:05.994720Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=processor.feature_extractor,\n    args=training_args,\n    compute_metrics=compute_cer,\n    train_dataset=train_data,\n    eval_dataset=valid_data,\n    data_collator=default_data_collator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:28:05.996239Z","iopub.execute_input":"2025-03-13T08:28:05.996521Z","iopub.status.idle":"2025-03-13T08:28:07.471558Z","shell.execute_reply.started":"2025-03-13T08:28:05.996493Z","shell.execute_reply":"2025-03-13T08:28:07.470934Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/trocr/processing_trocr.py:137: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!wandb login \"2edc2b81bac100efe83bb2cdf9200197908144c7\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:28:07.472342Z","iopub.execute_input":"2025-03-13T08:28:07.472541Z","iopub.status.idle":"2025-03-13T08:28:09.732371Z","shell.execute_reply.started":"2025-03-13T08:28:07.472523Z","shell.execute_reply":"2025-03-13T08:28:09.731398Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T08:28:09.733434Z","iopub.execute_input":"2025-03-13T08:28:09.733778Z","execution_failed":"2025-03-13T09:58:19.511Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfairuzalaila\u001b[0m (\u001b[33mfairuzalaila-university-of-dhaka\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250313_082816-t7zjageh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/fairuzalaila-university-of-dhaka/trocr-handwriting/runs/t7zjageh' target=\"_blank\">trocr-small-handwritten-v1</a></strong> to <a href='https://wandb.ai/fairuzalaila-university-of-dhaka/trocr-handwriting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/fairuzalaila-university-of-dhaka/trocr-handwriting' target=\"_blank\">https://wandb.ai/fairuzalaila-university-of-dhaka/trocr-handwriting</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/fairuzalaila-university-of-dhaka/trocr-handwriting/runs/t7zjageh' target=\"_blank\">https://wandb.ai/fairuzalaila-university-of-dhaka/trocr-handwriting/runs/t7zjageh</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6175' max='10290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 6175/10290 1:29:53 < 59:55, 1.14 it/s, Epoch 6/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Cer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.571900</td>\n      <td>1.584351</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.386200</td>\n      <td>1.301809</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.985700</td>\n      <td>0.835290</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.613800</td>\n      <td>0.596932</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.396500</td>\n      <td>0.449284</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='2' max='515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/515 00:00 < 04:33, 1.88 it/s]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1338: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Error computing CER: After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Error computing CER: After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word.\nError computing CER: After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word.\nError computing CER: After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word.\nError computing CER: After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import random\nimport matplotlib.pyplot as plt\n\ndef process_image(image):\n    device = next(model.parameters()).device\n    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n    generated_ids = model.generate(pixel_values)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return generated_text\n\nfor i in range(10):\n    idx = random.randint(0, len(valid_df) - 1)\n    path = train_data.root_dir + '/' + valid_df['file'].iloc[idx]\n    image = Image.open(path).convert(\"RGB\")    \n    # image.show()\n    print(process_image(image))\n    plt.imshow(image)\n    plt.axis(\"off\")  # Turn off axis for better visibility\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-13T09:58:19.512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = Image.open(path).convert(\"RGB\")\nimage","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-13T09:58:19.512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel, TrOCRProcessor\nfrom PIL import Image\n \n# Load model and processor from the Hub\nmodel = VisionEncoderDecoderModel.from_pretrained(\"Partha11/trocr-vit-model\")\nprocessor = TrOCRProcessor.from_pretrained(\"Partha11/trocr-vit-processor\")\n \n# Load Image and Process\nimage = Image.open(\"/kaggle/input/handwriting-dataset-v2/images/100_1_10_1.jpg\").convert(\"RGB\")\npixel_values = processor(image, return_tensors=\"pt\").pixel_values\n \n# Generate prediction\noutput_ids = model.generate(pixel_values)\npredicted_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n \nprint(predicted_text)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-13T09:58:19.512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reload the model and weights saved from the previous epochs\nmodel.load_weights('model_checkpoint.h5')\n\n# Continue training from epoch 5 (set initial_epoch to 5)\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, \n          callbacks=[checkpoint_callback], initial_epoch=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:00:00.715717Z","iopub.execute_input":"2025-03-13T10:00:00.716079Z","iopub.status.idle":"2025-03-13T10:00:00.731588Z","shell.execute_reply.started":"2025-03-13T10:00:00.716049Z","shell.execute_reply":"2025-03-13T10:00:00.730411Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-02a04c6ac41d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reload the model and weights saved from the previous epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_checkpoint.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Continue training from epoch 5 (set initial_epoch to 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.fit(X_train, y_train, epochs=10, batch_size=32, \n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}